---
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a PhD student at the Institute of Information Engineering, Chinese Academy of Sciences (CAS), specializing in the research of large language models (LLMs). I am expected to graduate in July 2025.

My research focuses on model compression and acceleration, specifically in the context of LLMs. I am interested in utilizing techniques like quantization, knowledge distillation, and pruning to improve computational efficiency and performance of LLMs. My ultimate goal is to achieve efficient language models.


## Selected Papers [[Full List](https://xyangyan.github.io/publications/)] [[Google Scholar](https://scholar.google.com/citations?user=gDJkRzwAAAAJ&hl)] 
* A Contrastive Self-distillation BERT with Kernel Alignment-Based Inference. 
[[pdf]](https://www.researchgate.net/publication/372006456_A_Contrastive_Self-distillation_BERT_with_Kernel_Alignment-Based_Inference)
[[code]](https://github.com/xyangyan/CsdBERT) <br>
<i>International Conference on Computational Science </i> (**ICCS**), 2023. <b>IIE-B Conference</b>.

* MetaBERT: Collaborative Meta-Learning for Accelerating BERT Inference.
[[pdf]](https://www.researchgate.net/publication/371825485_MetaBERT_Collaborative_Meta-Learning_for_Accelerating_BERT_Inference)
[[code]](https://github.com/xyangyan/MetaBERT) <br>
<i>International Conference on Computer Supported Cooperative Work in Design </i> (**CSCWD**), 2023. <b>CCF-C Conference</b>. 


## Honors and Awards
* UCAS Graduate Study Scholarship First Prize, 2022.
* UCAS Merit Student, 2022~2023.
* UCAS Merit Student, 2021~2022.


## Academic Service
* Reviewers of Conferences: ICASSP 2024.
