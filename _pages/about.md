---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a PhD student at the [Institute of Information Engineering](http://www.iie.ac.cn/), [Chinese Academy of Sciences (CAS)](https://www.cas.cn/), specializing in the research of large language models (LLMs). I am expected to graduate in July 2025.

My research focuses on model compression and acceleration, specifically in the context of LLMs. I am interested in utilizing techniques like quantization, knowledge distillation, and pruning to improve computational efficiency and performance of LLMs. My ultimate goal is to achieve efficient language models.


# üî• News
- *2024.03.30*: &nbsp;üéâüéâ Feeling tired and sleepy, it's time for a relaxing moment. ‚òû[FunAI](https://funai.vip)


## üìù Selected Papers [[Full List](https://xyangyan.github.io/publications/)] [[Google Scholar](https://scholar.google.com/citations?user=gDJkRzwAAAAJ&hl)] 

* Efficient One-Shot Pruning of Large Language Models with Low-Rank Approximation.
[[pdf]](https://github.com/xyangyan/Eplra)
[[code]](https://github.com/xyangyan/Eplra) <br>
<i>IEEE International Conference on Systems, Man, and Cybernetics </i> (**SMC**), 2024. <b>CCF-C Conference</b>.

* Data-Efficient Knowledge Distillation with Teacher Assistant-Based Dynamic Objective Alignment.
[[pdf]](https://www.researchgate.net/publication/381777246_Data-Efficient_Knowledge_Distillation_with_Teacher_Assistant-Based_Dynamic_Objective_Alignment)
[[code]](https://github.com/xyangyan/DeKD.git) <br>
<i>International Conference on Computational Science </i> (**ICCS**), 2024. <b>IIE-B Conference</b>.


* A Contrastive Self-distillation BERT with Kernel Alignment-Based Inference. 
[[pdf]](https://www.researchgate.net/publication/372006456_A_Contrastive_Self-distillation_BERT_with_Kernel_Alignment-Based_Inference)
[[code]](https://github.com/xyangyan/CsdBERT) <br>
<i>International Conference on Computational Science </i> (**ICCS**), 2023. <b>IIE-B Conference</b>.

* MetaBERT: Collaborative Meta-Learning for Accelerating BERT Inference.
[[pdf]](https://www.researchgate.net/publication/371825485_MetaBERT_Collaborative_Meta-Learning_for_Accelerating_BERT_Inference)
[[code]](https://github.com/xyangyan/MetaBERT) <br>
<i>International Conference on Computer Supported Cooperative Work in Design </i> (**CSCWD**), 2023. <b>CCF-C Conference</b>.


## üéñ Honors and Awards
* PhD Graduate Study Scholarship First Prize, UCAS, 2022.
* Merit Student, UCAS, 2022~2023.
* Merit Student, UCAS, 2021~2022.

## üìñ Teaching
* Machine Learning, Teaching Assistant, UCAS.
* Pattern Matching and Information Filtering, Teaching Assistant, UCAS.

## Academic Service
* Reviewers of Conferences: ICASSP 2024.
